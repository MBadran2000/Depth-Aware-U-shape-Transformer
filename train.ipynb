{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hello\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "import pytorch_ssim\n",
    "import  time \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn.modules.loss import _Loss \n",
    "from net.Ushape_Trans import *\n",
    "#from dataset import prepare_data, Dataset\n",
    "from net.utils import *\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from utility import plots as plots, ptcolor as ptcolor, ptutils as ptutils, data as data\n",
    "from loss.LAB import *\n",
    "from loss.LCH import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(img):\n",
    "    output=[]\n",
    "    output.append(F.interpolate(img, scale_factor=0.125))\n",
    "    output.append(F.interpolate(img, scale_factor=0.25))\n",
    "    output.append(F.interpolate(img, scale_factor=0.5))\n",
    "    output.append(img)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "def split4(img):\n",
    "    output=[]\n",
    "    output.append(F.interpolate(img, scale_factor=0.125))\n",
    "    output.append(F.interpolate(img, scale_factor=0.25))\n",
    "    output.append(F.interpolate(img, scale_factor=0.5))\n",
    "    output.append(F.interpolate(img, scale_factor=1))\n",
    "\n",
    "    output.append(img)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = 'float32'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x=[]#train 1 tomorrow\n",
    "\n",
    "path='./data/input/'#要改\n",
    "pathdepth='./data/depth/'#要改\n",
    "\n",
    "path_list = os.listdir(path)\n",
    "# path_list.sort(key=lambda x:int(x.split('.')[0]))\n",
    "path_list.sort(key=lambda x:(x.split('.')[0]))\n",
    "\n",
    "testequal = []\n",
    "i= 0\n",
    "\n",
    "for item in path_list:\n",
    "    i+=1\n",
    "    if i==199:\n",
    "        testequal.append(item)\n",
    "        i=0\n",
    "    \n",
    "    impath=path+item\n",
    "#     print(\"开始处理\"+impath)\n",
    "    imgx= cv2.imread(path+item)\n",
    "    imgx = cv2.cvtColor(imgx, cv2.COLOR_BGR2RGB)\n",
    "    imgx=cv2.resize(imgx,(256,256))\n",
    "    \n",
    "    imgd= cv2.imread(pathdepth+item.split(\".\")[0]+\".png\",0)\n",
    "    imgd= cv2.resize(imgd,(256,256)).reshape((256,256,1))\n",
    "    imgd = (imgd  / np.max(imgd))* 255\n",
    "    imgx = np.concatenate((imgx, imgd), axis=2)\n",
    "    \n",
    "    training_x.append(imgx)   \n",
    "\n",
    "X_train = []\n",
    "\n",
    "for features in training_x:\n",
    "    X_train.append(features)\n",
    "    \n",
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import path\n",
    "\n",
    "# for item in path_list:\n",
    "#     if(not path.exists(pathdepth+item.split(\".\")[0]+\".png\")):\n",
    "#         print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imgx= cv2.imread(\"./data/input/0001.png\")\n",
    "# imgx = cv2.cvtColor(imgx, cv2.COLOR_BGR2RGB)\n",
    "# imgx=cv2.resize(imgx,(256,256))\n",
    "\n",
    "# imgd= cv2.imread(\"./data/depth/0001.png\",0)\n",
    "# imgd= cv2.resize(imgd,(256,256)).reshape((256,256,1))\n",
    "# imgd = (imgd  / np.max(imgd))* 255\n",
    "\n",
    "# # print(imgx.shape,imgd.shape)\n",
    "# pp=np.concatenate((imgx, imgd), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11170, 256, 256, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.astype(dtype)\n",
    "X_train= torch.from_numpy(X_train)\n",
    "X_train=X_train.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11170, 4, 256, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=X_train/255.0\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_y=[]\n",
    "path='./data/GT/'#要改\n",
    "path_list = os.listdir(path)\n",
    "# path_list.sort(key=lambda x:int(x.split('.')[0]))\n",
    "path_list.sort(key=lambda x:(x.split('.')[0]))\n",
    "\n",
    "\n",
    "i= 0\n",
    "j=0\n",
    "for item in path_list:\n",
    "    i+=1\n",
    "    if i==199:\n",
    "        if(testequal[j]!= item):\n",
    "            print(\"Error\")\n",
    "        i=0\n",
    "        j+=1\n",
    "    \n",
    "    impath=path+item\n",
    "#     print(\"开始处理\"+impath)\n",
    "    imgx= cv2.imread(path+item)\n",
    "    imgx = cv2.cvtColor(imgx, cv2.COLOR_BGR2RGB)\n",
    "    imgx=cv2.resize(imgx,(256,256))\n",
    "    training_y.append(imgx)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "y_train = []\n",
    "\n",
    "for features in training_y:\n",
    "    y_train.append(features)\n",
    "    \n",
    "\n",
    "    \n",
    "y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_y=0\n",
    "testequal=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.astype(dtype)\n",
    "y_train= torch.from_numpy(y_train)\n",
    "y_train=y_train.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11170, 3, 256, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=y_train/255.0\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_x=[]\n",
    "path='./test/input/'#要改\n",
    "pathdepth = \"./test/depth/\"\n",
    "path_list = os.listdir(path)\n",
    "# path_list.sort(key=lambda x:int(x.split('.')[0]))\n",
    "path_list.sort(key=lambda x:(x.split('.')[0]))\n",
    "\n",
    "for item in path_list:\n",
    "    impath=path+item\n",
    "    #print(\"开始处理\"+impath)\n",
    "    imgx= cv2.imread(path+item)\n",
    "    if imgx is None:\n",
    "        print(path+item)\n",
    "        continue\n",
    "    imgx = cv2.cvtColor(imgx, cv2.COLOR_BGR2RGB)\n",
    "    imgx=cv2.resize(imgx,(256,256))\n",
    "    \n",
    "    imgd= cv2.imread(pathdepth+item.split(\".\")[0]+\".png\",0)\n",
    "    imgd= cv2.resize(imgd,(256,256)).reshape((256,256,1))\n",
    "    imgd = (imgd  / np.max(imgd))* 255\n",
    "    imgx = np.concatenate((imgx, imgd), axis=2)\n",
    "    \n",
    "    test_x.append(imgx)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "x_test = []\n",
    "\n",
    "for features in test_x:\n",
    "    x_test.append(features)\n",
    "    \n",
    "\n",
    "    \n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x_test.astype(dtype)\n",
    "x_test= torch.from_numpy(x_test)\n",
    "x_test=x_test.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1214, 4, 256, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y=[]\n",
    "path='./test/GT/'#要改\n",
    "path_list = os.listdir(path)\n",
    "# path_list.sort(key=lambda x:int(x.split('.')[0]))\n",
    "path_list.sort(key=lambda x:(x.split('.')[0]))\n",
    "\n",
    "for item in path_list:\n",
    "    impath=path+item\n",
    "    #print(\"开始处理\"+impath)\n",
    "    imgx= cv2.imread(path+item)\n",
    "    imgx = cv2.cvtColor(imgx, cv2.COLOR_BGR2RGB)\n",
    "    imgx=cv2.resize(imgx,(256,256))\n",
    "    test_Y.append(imgx)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "Y_test = []\n",
    "\n",
    "for features in test_Y:\n",
    "    Y_test.append(features)\n",
    "    \n",
    "\n",
    "    \n",
    "Y_test = np.array(Y_test)\n",
    "#X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=Y_test.astype(dtype)\n",
    "Y_test= torch.from_numpy(Y_test)\n",
    "Y_test=Y_test.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=Y_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1214, 3, 256, 256])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as dataf\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "dataset = dataf.TensorDataset(X_train,y_train)\n",
    "loader = dataf.DataLoader(dataset, batch_size=1, shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hello\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "c:\\users\\hello\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\users\\hello\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lch_Loss()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_GAN=nn.MSELoss(size_average=False)\n",
    "criterion_pixelwise=nn.MSELoss(size_average=False)\n",
    "MSE = nn.MSELoss(size_average=False)\n",
    "SSIM = pytorch_ssim.SSIM()\n",
    "L_vgg = VGG19_PercepLoss()\n",
    "L_lab=lab_Loss()\n",
    "L_lch=lch_Loss()\n",
    "\n",
    "criterion_pixelwise.cuda()\n",
    "L_vgg.cuda()\n",
    "MSE.cuda()\n",
    "SSIM.cuda()\n",
    "criterion_GAN.cuda()\n",
    "L_lab.cuda()\n",
    "L_lch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel=0.1\n",
    "lambda_lab=0.001\n",
    "lambda_lch=1\n",
    "lambda_con = 100\n",
    "lambda_ssim=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, 256 // 2 ** 5, 256// 2 ** 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator().cuda()\n",
    "discriminator = Discriminator().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loading epoch 160 成功！\n"
     ]
    }
   ],
   "source": [
    "# 如果有保存的模型，则加载模型，并在其基础上继续训练\n",
    "use_pretrain=True\n",
    "if use_pretrain:\n",
    "    # Load pretrained models\n",
    "    start_epoch=160\n",
    "    generator.load_state_dict(torch.load(\"saved_models/G/generator_%d.pth\" % (start_epoch)))\n",
    "    discriminator.load_state_dict(torch.load(\"saved_models/D/discriminator_%d.pth\" % (start_epoch)))\n",
    "    print('successfully loading epoch {} 成功！'.format(start_epoch))\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    print('No pretrain model found, training will start from scratch！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    generator.eval()\n",
    "    i=random.randrange(1,90)\n",
    "    real_A = Variable(x_test[i,:,:,:]).cuda()\n",
    "    real_B = Variable(Y_test[i,:,:,:]).cuda()\n",
    "    real_A=real_A.unsqueeze(0)\n",
    "    real_B=real_B.unsqueeze(0)\n",
    "    fake_B = generator(real_A)\n",
    "    #print(fake_B.shape)\n",
    "    imgx=fake_B[3].data\n",
    "    imgy=real_B.data\n",
    "    x=imgx[:,:,:,:]\n",
    "    y=imgy[:,:,:,:]\n",
    "    img_sample = torch.cat((x,y), -2)\n",
    "    save_image(img_sample, \"images/%s/%s.png\" % ('results', batches_done), nrow=5, normalize=True)#要改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "LR=0.0005\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LR,  betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR,  betas=(0.5, 0.999))\n",
    "scheduler_G=lr_scheduler.StepLR(optimizer_G,step_size=40,gamma=0.8)\n",
    "scheduler_D=lr_scheduler.StepLR(optimizer_D,step_size=40,gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 170/300] [Batch 66/11170][PSNR: 22.071240] [SSIM: 0.774581] [D loss: 0.016711] [G loss: 104.821472],[lab: 44.235352],[lch: 11.941713], [pixel: 41.190018],[VGG_loss: 21.668670], [adv: 63.243813] ETA: 2 days, 9:44:39.91718958015015"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [47], line 169\u001b[0m\n\u001b[0;32m    167\u001b[0m batches_left \u001b[38;5;241m=\u001b[39m n_epochs \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader) \u001b[38;5;241m-\u001b[39m batches_done\n\u001b[0;32m    168\u001b[0m out_train\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(fake_B[\u001b[38;5;241m3\u001b[39m], \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1.\u001b[39m) \n\u001b[1;32m--> 169\u001b[0m psnr_train \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_PSNR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreal_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m time_left \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(seconds\u001b[38;5;241m=\u001b[39mbatches_left \u001b[38;5;241m*\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m prev_time))\n\u001b[0;32m    171\u001b[0m prev_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\Desktop\\u-transformer\\U-shape_Transformer_for_Underwater_Image_Enhancement-main\\net\\utils.py:47\u001b[0m, in \u001b[0;36mbatch_PSNR\u001b[1;34m(img, imclean, data_range)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_PSNR\u001b[39m(img, imclean, data_range):\n\u001b[1;32m---> 47\u001b[0m     Img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     48\u001b[0m     Iclean \u001b[38;5;241m=\u001b[39m imclean\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     49\u001b[0m     PSNR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "import time as time\n",
    "import datetime\n",
    "import sys\n",
    "from torchvision.utils import save_image\n",
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "f1 = open('psnr.csv','w',encoding='utf-8')#要改\n",
    "csv_writer1 = csv.writer(f1)\n",
    "f2 = open('SSIM.csv','w',encoding='utf-8')#要改\n",
    "csv_writer2 = csv.writer(f2)\n",
    "\n",
    "checkpoint_interval=1\n",
    "epochs=start_epoch\n",
    "n_epochs=300\n",
    "sample_interval=1000\n",
    "\n",
    "# ingnored when opt.mode=='S'\n",
    "psnr_list = [] \n",
    "prev_time = time.time()\n",
    "\n",
    "for epoch in range(epochs,n_epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "\n",
    "        # Model inputs\n",
    "        real_A = Variable(batch[0]).cuda() \n",
    "        real_B = Variable(batch[1]).cuda()\n",
    "        #real_A1=Tensor(np.array(split(real_A)).astype(dtype))\n",
    "        #real_B1=Tensor(np.array(split(real_B)).astype(dtype))\n",
    "        real_A1=split(real_A[:,1:,:,:])\n",
    "        real_B1=split(real_B)\n",
    "        #print(len(real_A1))\n",
    "        #print(len(real_B1))\n",
    "        #real_A1= torch.tensor([item.cpu().detach().numpy() for item in real_A1]).cuda() \n",
    "        #real_B1= torch.tensor([item.cpu().detach().numpy() for item in real_B1]).cuda() \n",
    "\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)#全1\n",
    "        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)#全0\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = generator(real_A)\n",
    "        \n",
    "#         print(len(fake_B))\n",
    "#         print(fake_B[0].shape)\n",
    "#         print(fake_B[1].shape)\n",
    "#         print(fake_B[2].shape)\n",
    "#         print(fake_B[3].shape)\n",
    "        \n",
    "#         print(len(real_A1))\n",
    "#         print(real_A1[0].shape)\n",
    "#         print(real_A1[1].shape)\n",
    "#         print(real_A1[2].shape)\n",
    "#         print(real_A1[3].shape)\n",
    "        \n",
    "        #fake_B1 = list(map(lambda x: x.detach(), fake_B))\n",
    "        pred_fake = discriminator(fake_B, real_A1)\n",
    "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "        # Pixel-wise loss\n",
    "#         print(len(fake_B))\n",
    "#         print(fake_B[0].shape)\n",
    "#         print(real_B1[0].shape)\n",
    "\n",
    "#         fake3 = fake_B.copy()\n",
    "#         fake_B[0] = fake_B[0][:,1:,:,:]\n",
    "#         fake_B[1] = fake_B[1][:,1:,:,:]\n",
    "#         fake_B[2] = fake_B[2][:,1:,:,:]\n",
    "#         fake_B[3] = fake_B[3][:,1:,:,:]\n",
    "\n",
    "#         print(len(fake_B))\n",
    "#         print(fake_B[0].shape)\n",
    "#         print(fake_B[1].shape)\n",
    "#         print(fake_B[2].shape)\n",
    "#         print(fake_B[3].shape)\n",
    "        \n",
    "#         print(len(real_B1))\n",
    "#         print(real_B1[0].shape)\n",
    "#         print(real_B1[1].shape)\n",
    "#         print(real_B1[2].shape)\n",
    "#         print(real_B1[3].shape)\n",
    "\n",
    "        loss_pixel = (criterion_pixelwise(fake_B[0], real_B1[0])+\\\n",
    "                      criterion_pixelwise(fake_B[1], real_B1[1])+\\\n",
    "                      criterion_pixelwise(fake_B[2], real_B1[2])+\\\n",
    "                      criterion_pixelwise(fake_B[3], real_B1[3]))/4.0\n",
    "        loss_ssim=   -(SSIM(fake_B[0], real_B1[0])+\\\n",
    "                      SSIM(fake_B[1], real_B1[1])+\\\n",
    "                      SSIM(fake_B[2], real_B1[2])+\\\n",
    "                      SSIM(fake_B[3], real_B1[3]))/4.0\n",
    "        ssim_value = - loss_ssim.item()\n",
    "        loss_con = (L_vgg(fake_B[0], real_B1[0])+\\\n",
    "                    L_vgg(fake_B[1], real_B1[1])+\\\n",
    "                    L_vgg(fake_B[2], real_B1[2])+\\\n",
    "                    L_vgg(fake_B[3], real_B1[3]))/4.0\n",
    "        loss_lab = (L_lab(fake_B[0], real_B1[0])+\\\n",
    "                    L_lab(fake_B[1], real_B1[1])+\\\n",
    "                    L_lab(fake_B[2], real_B1[2])+\\\n",
    "                    L_lab(fake_B[3], real_B1[3]))/4.0\n",
    "        loss_lch = (L_lch(fake_B[0], real_B1[0])+\\\n",
    "                    L_lch(fake_B[1], real_B1[1])+\\\n",
    "                    L_lch(fake_B[2], real_B1[2])+\\\n",
    "                    L_lch(fake_B[3], real_B1[3]))/4.0     \n",
    "        \n",
    "\n",
    "        # Total loss\n",
    "        loss_G = (loss_GAN + lambda_pixel * loss_pixel+lambda_ssim*loss_ssim+\\\n",
    "        lambda_con*loss_con+lambda_lab*loss_lab+lambda_lch*loss_lch)\n",
    "\n",
    "        loss_G.backward(retain_graph=True)\n",
    "\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "#         fake_B = fake3\n",
    "#         print(len(fake_B))\n",
    "#         print(real_B1[0].shape)\n",
    "#         print(real_B1[1].shape)\n",
    "#         print(real_B1[2].shape)\n",
    "#         print(real_B1[3].shape)\n",
    "        \n",
    "#         print(len(real_A1))\n",
    "#         print(real_A1[0].shape)\n",
    "#         print(real_A1[1].shape)\n",
    "#         print(real_A1[2].shape)\n",
    "#         print(real_A1[3].shape)\n",
    "        # Real loss\n",
    "        pred_real = discriminator(real_B1, real_A1)\n",
    "        loss_real = criterion_GAN(pred_real, valid)\n",
    "\n",
    "        # Fake loss\n",
    "        fake_B[0]=fake_B[0].detach()\n",
    "        fake_B[1]=fake_B[1].detach()\n",
    "        fake_B[2]=fake_B[2].detach()\n",
    "        fake_B[3]=fake_B[3].detach()\n",
    "        pred_fake1 = discriminator(fake_B, real_A1)\n",
    "        loss_fake = criterion_GAN(pred_fake1, fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "        #loss_D=loss_real\n",
    "\n",
    "        loss_D.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(loader) + i\n",
    "        batches_left = n_epochs * len(loader) - batches_done\n",
    "        out_train= torch.clamp(fake_B[3], 0., 1.) \n",
    "        psnr_train = batch_PSNR(out_train,real_B, 1.)\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d][PSNR: %f] [SSIM: %f] [D loss: %f] [G loss: %f],[lab: %f],[lch: %f], [pixel: %f],[VGG_loss: %f], [adv: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                n_epochs,\n",
    "                i,\n",
    "                len(loader),\n",
    "                psnr_train,\n",
    "                ssim_value,\n",
    "                loss_D.item(),\n",
    "                loss_G.item(),\n",
    "                0.001*loss_lab.item(),\n",
    "                1*loss_lch.item(),\n",
    "                0.1*loss_pixel.item(),\n",
    "                100*loss_con.item(),\n",
    "                loss_GAN.item(),\n",
    "                \n",
    "                \n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_images(batches_done)\n",
    "            csv_writer1.writerow([str(psnr_train)])\n",
    "            csv_writer2.writerow([str(ssim_value)])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(generator.state_dict(), \"saved_models/G/generator_%d.pth\" % (epoch))\n",
    "        torch.save(discriminator.state_dict(), \"saved_models/D/discriminator_%d.pth\" % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import net.Ushape_Trans  as un\n",
    "# import importlib\n",
    "# importlib.reload(un)\n",
    "# un.Generator().out_ch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = un.Generator().cuda()\n",
    "# discriminator = un.Discriminator().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import peak_signal_noise_ratio\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator().in_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Generator' object has no attribute 'num_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\hello\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Generator' object has no attribute 'num_layers'"
     ]
    }
   ],
   "source": [
    "Generator().num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_B[0][:,1:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_B1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test[0].permute(1,2,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
